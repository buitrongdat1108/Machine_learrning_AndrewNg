WEBVTT

1
00:00:01.520 --> 00:00:04.110
In the last video,
you saw how to

2
00:00:04.110 --> 00:00:06.510
implement forward
prop in Python,

3
00:00:06.510 --> 00:00:10.380
but by hard coding lines of
code for every single neuron.

4
00:00:10.380 --> 00:00:11.790
Let's now take a look at

5
00:00:11.790 --> 00:00:13.365
the more general implementation

6
00:00:13.365 --> 00:00:15.315
of forward prop in Python.

7
00:00:15.315 --> 00:00:17.115
Similar to the previous video,

8
00:00:17.115 --> 00:00:20.310
my goal in this video is to
show you the code so that

9
00:00:20.310 --> 00:00:23.490
when you see it again
in their practice lab,

10
00:00:23.490 --> 00:00:26.430
in the optional labs, you
know how to interpret it.

11
00:00:26.430 --> 00:00:28.725
As we walk through this example,

12
00:00:28.725 --> 00:00:29.790
don't worry about taking

13
00:00:29.790 --> 00:00:31.665
notes on every
single line of code.

14
00:00:31.665 --> 00:00:33.635
If you can read
through the code and

15
00:00:33.635 --> 00:00:36.290
understand it, that's
definitely enough.

16
00:00:36.290 --> 00:00:37.880
What you can do is write

17
00:00:37.880 --> 00:00:41.225
a function to implement
a dense layer,

18
00:00:41.225 --> 00:00:44.650
that is a single layer
of a neural network.

19
00:00:44.650 --> 00:00:47.040
I'm going to define
the dense function,

20
00:00:47.040 --> 00:00:48.755
which takes as input

21
00:00:48.755 --> 00:00:51.545
the activation from
the previous layer,

22
00:00:51.545 --> 00:00:54.440
as well as the parameters w and

23
00:00:54.440 --> 00:00:58.325
b for the neurons
in a given layer.

24
00:00:58.325 --> 00:01:01.805
Using the example from
the previous video,

25
00:01:01.805 --> 00:01:06.860
if layer 1 has three neurons,

26
00:01:06.860 --> 00:01:13.610
and if w_1 and w_2
and w_3 are these,

27
00:01:13.610 --> 00:01:15.830
then what we'll do is

28
00:01:15.830 --> 00:01:19.699
stack all of these wave
vectors into a matrix.

29
00:01:19.699 --> 00:01:25.130
This is going to be a
two by three matrix,

30
00:01:25.130 --> 00:01:27.350
where the first column is

31
00:01:27.350 --> 00:01:30.725
the parameter w_1,1
the second column

32
00:01:30.725 --> 00:01:31.910
is the parameter w_1,

33
00:01:31.910 --> 00:01:36.440
2, and the third column
is the parameter w_1,3.

34
00:01:36.440 --> 00:01:38.495
Then in a similar way,

35
00:01:38.495 --> 00:01:42.255
if you have parameters be,

36
00:01:42.255 --> 00:01:44.250
b_1,1 equals negative one,

37
00:01:44.250 --> 00:01:46.500
b_1,2 equals one, and so on,

38
00:01:46.500 --> 00:01:49.370
then we're going to stack
these three numbers into

39
00:01:49.370 --> 00:01:52.535
a 1D array b as follows,

40
00:01:52.535 --> 00:01:54.685
negative one, one, two.

41
00:01:54.685 --> 00:01:57.470
What the dense function
will do is take as

42
00:01:57.470 --> 00:02:00.230
inputs the activation
from the previous layer,

43
00:02:00.230 --> 00:02:02.555
and a here could be a_0,

44
00:02:02.555 --> 00:02:04.715
which is equal to x,

45
00:02:04.715 --> 00:02:07.250
or the activation
from a later layer,

46
00:02:07.250 --> 00:02:12.890
as well as the w parameters
stacked in columns,

47
00:02:12.890 --> 00:02:14.120
like shown on the right,

48
00:02:14.120 --> 00:02:16.325
as well as the b parameters

49
00:02:16.325 --> 00:02:19.645
also stacked into a 1D array,

50
00:02:19.645 --> 00:02:22.865
like shown to the
left over there.

51
00:02:22.865 --> 00:02:28.150
What this function
would do is input a to

52
00:02:28.150 --> 00:02:30.260
activation from the
previous layer and will

53
00:02:30.260 --> 00:02:34.015
output the activations
from the current layer.

54
00:02:34.015 --> 00:02:37.440
Let's step through the
code for doing this.

55
00:02:37.440 --> 00:02:41.310
Here's the code. First,
units equals W.shape,1.

56
00:02:41.310 --> 00:02:47.660
W here is a two-by-three matrix,

57
00:02:47.660 --> 00:02:50.375
and so the number of
columns is three.

58
00:02:50.375 --> 00:02:53.600
That's equal to the number
of units in this layer.

59
00:02:53.600 --> 00:02:56.440
Here, units would
be equal to three.

60
00:02:56.440 --> 00:02:58.585
Looking at the shape of w,

61
00:02:58.585 --> 00:03:01.310
is just a way of pulling
out the number of

62
00:03:01.310 --> 00:03:05.335
hidden units or the number
of units in this layer.

63
00:03:05.335 --> 00:03:08.570
Next, we set a to be an array of

64
00:03:08.570 --> 00:03:12.155
zeros with as many elements
as there are units.

65
00:03:12.155 --> 00:03:14.180
In this example, we need

66
00:03:14.180 --> 00:03:16.205
to output three
activation values,

67
00:03:16.205 --> 00:03:19.620
so this just initializes
a to be zero,

68
00:03:19.620 --> 00:03:22.800
zero, zero, an array
of three zeros.

69
00:03:22.800 --> 00:03:26.430
Next, we go through a for
loop to compute the first,

70
00:03:26.430 --> 00:03:28.600
second, and third elements of a.

71
00:03:28.600 --> 00:03:30.350
For j in range units,

72
00:03:30.350 --> 00:03:33.665
so j goes from zero
to units minus one.

73
00:03:33.665 --> 00:03:34.850
It goes from 0, 1,

74
00:03:34.850 --> 00:03:37.805
2 indexing from zero
and Python as usual.

75
00:03:37.805 --> 00:03:42.915
This command w equals
W colon comma j,

76
00:03:42.915 --> 00:03:45.075
this is how you pull out

77
00:03:45.075 --> 00:03:49.790
the jth column of a
matrix in Python.

78
00:03:49.790 --> 00:03:52.115
The first time
through this loop,

79
00:03:52.115 --> 00:03:55.325
this will pull the
first column of w,

80
00:03:55.325 --> 00:03:58.550
and so will pull out w_1,1.

81
00:03:58.550 --> 00:04:00.430
The second time
through this loop,

82
00:04:00.430 --> 00:04:03.410
when you're computing the
activation of the second unit,

83
00:04:03.410 --> 00:04:06.920
will pull out the second
column corresponding to w_1,

84
00:04:06.920 --> 00:04:10.780
2, and so on for the third
time through this loop.

85
00:04:10.780 --> 00:04:14.335
Then you compute z using
the usual formula,

86
00:04:14.335 --> 00:04:16.040
is a dot product between

87
00:04:16.040 --> 00:04:18.095
that parameter w and

88
00:04:18.095 --> 00:04:20.480
the activation that
you have received,

89
00:04:20.480 --> 00:04:22.610
plus b, j.

90
00:04:22.610 --> 00:04:25.430
And then you compute
the activation a, j,

91
00:04:25.430 --> 00:04:29.035
equals g sigmoid
function applied to z.

92
00:04:29.035 --> 00:04:31.220
Three times through this
loop and you compute it,

93
00:04:31.220 --> 00:04:33.455
the values for all three values

94
00:04:33.455 --> 00:04:35.705
of this vector of
activation is a.

95
00:04:35.705 --> 00:04:38.860
Then finally you return a.

96
00:04:38.860 --> 00:04:41.375
What the dense
function does is it

97
00:04:41.375 --> 00:04:43.880
inputs the activations
from the previous layer,

98
00:04:43.880 --> 00:04:46.430
and given the parameters
for the current layer,

99
00:04:46.430 --> 00:04:50.360
it returns the activations
for the next layer.

100
00:04:50.360 --> 00:04:52.055
Given the dense function,

101
00:04:52.055 --> 00:04:53.330
here's how you can string

102
00:04:53.330 --> 00:04:56.780
together a few dense
layers sequentially,

103
00:04:56.780 --> 00:04:59.990
in order to implement forward
prop in the neural network.

104
00:04:59.990 --> 00:05:03.545
Given the input features x,

105
00:05:03.545 --> 00:05:05.460
you can then compute

106
00:05:05.460 --> 00:05:11.900
the activations a_1 to be
a_1 equals dense of x,

107
00:05:11.900 --> 00:05:14.490
w_1, b_1, where here w_1,

108
00:05:14.490 --> 00:05:16.565
b_1 are the parameters,

109
00:05:16.565 --> 00:05:18.365
sometimes also
called the weights

110
00:05:18.365 --> 00:05:20.785
of the first hidden layer.

111
00:05:20.785 --> 00:05:26.385
Then you can compute a_2
as dense of now a_1,

112
00:05:26.385 --> 00:05:28.870
which you just computed above.

113
00:05:29.030 --> 00:05:32.330
W_2, b-2 which are
the parameters or

114
00:05:32.330 --> 00:05:35.020
weights of this
second hidden layer.

115
00:05:35.020 --> 00:05:38.490
Then compute a_3 and a_4.

116
00:05:38.490 --> 00:05:42.080
If this is a neural
network with four layers,

117
00:05:42.080 --> 00:05:46.610
then define the output f
of x is just equal to a_4,

118
00:05:46.610 --> 00:05:49.550
and so you return f of x.

119
00:05:49.550 --> 00:05:53.195
Notice that here I'm using W,

120
00:05:53.195 --> 00:05:55.759
because under the
notational conventions

121
00:05:55.759 --> 00:05:58.490
from linear algebra is
to use uppercase or

122
00:05:58.490 --> 00:06:01.070
a capital alphabet is
when it's referring to

123
00:06:01.070 --> 00:06:05.630
a matrix and lowercase refer
to vectors and scalars.

124
00:06:05.630 --> 00:06:06.950
So because it's a matrix,

125
00:06:06.950 --> 00:06:09.990
this is W. That's it.

126
00:06:09.990 --> 00:06:11.540
You now know how to implement

127
00:06:11.540 --> 00:06:13.725
forward prop yourself
from scratch.

128
00:06:13.725 --> 00:06:17.000
You get to see all this code
and run it and practice it

129
00:06:17.000 --> 00:06:20.660
yourself in the practice lab
coming off to this as well.

130
00:06:20.660 --> 00:06:22.940
I think that even
when you're using

131
00:06:22.940 --> 00:06:25.100
powerful libraries
like TensorFlow,

132
00:06:25.100 --> 00:06:27.710
it's helpful to know how
it works under the hood.

133
00:06:27.710 --> 00:06:30.095
Because in case
something goes wrong,

134
00:06:30.095 --> 00:06:32.045
in case something
runs really slowly,

135
00:06:32.045 --> 00:06:33.830
or you have a strange result,

136
00:06:33.830 --> 00:06:35.585
or it looks like there's a bug,

137
00:06:35.585 --> 00:06:38.600
your ability to understand
what's actually going on

138
00:06:38.600 --> 00:06:40.130
will make you much
more effective

139
00:06:40.130 --> 00:06:41.690
when debugging your code.

140
00:06:41.690 --> 00:06:45.050
When I run machine learning
algorithms a lot of the time,

141
00:06:45.050 --> 00:06:46.490
frankly, it doesn't work.

142
00:06:46.490 --> 00:06:48.325
Sophie, not the first time.

143
00:06:48.325 --> 00:06:50.975
I find that my ability to debug

144
00:06:50.975 --> 00:06:53.840
my code to be a TensorFlow
code or something else,

145
00:06:53.840 --> 00:06:55.580
is really important to being

146
00:06:55.580 --> 00:06:58.195
an effective machine
learning engineer.

147
00:06:58.195 --> 00:06:59.945
Even when you're using

148
00:06:59.945 --> 00:07:02.015
TensorFlow or some
other framework,

149
00:07:02.015 --> 00:07:06.020
I hope that you find this
deeper understanding useful for

150
00:07:06.020 --> 00:07:08.570
your own applications
and for debugging

151
00:07:08.570 --> 00:07:11.980
your own machine learning
algorithms as well.

152
00:07:11.980 --> 00:07:15.545
That's it. That's the
last required video

153
00:07:15.545 --> 00:07:17.615
of this week with code in it.

154
00:07:17.615 --> 00:07:18.950
In the next video,

155
00:07:18.950 --> 00:07:20.720
I'd like to dive
into what I think is

156
00:07:20.720 --> 00:07:22.910
a fun and fascinating
topic, which is,

157
00:07:22.910 --> 00:07:25.715
what is the relationship
between neural networks and

158
00:07:25.715 --> 00:07:30.145
AI or AGI, artificial
general intelligence?

159
00:07:30.145 --> 00:07:32.045
This is a controversial topic,

160
00:07:32.045 --> 00:07:34.235
but because it's been
so widely discussed,

161
00:07:34.235 --> 00:07:37.270
I want to share with you
some thoughts on this.

162
00:07:37.270 --> 00:07:39.855
When you are asked,

163
00:07:39.855 --> 00:07:41.915
are neural networks at all

164
00:07:41.915 --> 00:07:44.180
on the path to human
level intelligence?

165
00:07:44.180 --> 00:07:47.555
You have a framework for
thinking about that question.

166
00:07:47.555 --> 00:07:49.970
Let's go take a look
at that fun topic,

167
00:07:49.970 --> 00:07:52.410
I think, in the next video.