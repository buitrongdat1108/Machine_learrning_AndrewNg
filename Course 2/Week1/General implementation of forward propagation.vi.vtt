WEBVTT
Kind: captions
Language: vi

00:00:01.520 --> 00:00:04.110
Trong video trước,
bạn đã thấy cách

00:00:04.110 --> 00:00:06.510
thực hiện chuyển tiếp
prop trong Python,

00:00:06.510 --> 00:00:10.380
mà bằng các dòng mã cứng
cho từng nơ-ron đơn lẻ.

00:00:10.380 --> 00:00:11.790
Bây giờ chúng ta hãy xem

00:00:11.790 --> 00:00:13.365
việc thực hiện tổng quát hơn

00:00:13.365 --> 00:00:15.315
của chỗ dựa phía trước trong Python.

00:00:15.315 --> 00:00:17.115
Tương tự như video trước,

00:00:17.115 --> 00:00:20.310
mục tiêu của tôi trong video này là
cho bạn thấy mã để

00:00:20.310 --> 00:00:23.490
khi bạn nhìn thấy nó một lần nữa
trong phòng thí nghiệm thực hành của họ,

00:00:23.490 --> 00:00:26.430
trong các phòng thí nghiệm tùy chọn, bạn
biết cách diễn giải nó.

00:00:26.430 --> 00:00:28.725
Khi chúng ta đi qua ví dụ này,

00:00:28.725 --> 00:00:29.790
đừng lo lắng về việc lấy

00:00:29.790 --> 00:00:31.665
ghi chú trên mỗi
dòng mã.

00:00:31.665 --> 00:00:33.635
Nếu bạn có thể đọc
qua mã và

00:00:33.635 --> 00:00:36.290
hiểu nó,
chắc chắn là đủ.

00:00:36.290 --> 00:00:37.880
Những gì bạn có thể làm là viết

00:00:37.880 --> 00:00:41.225
một chức năng để thực hiện
một lớp dày đặc,

00:00:41.225 --> 00:00:44.650
đó là một lớp duy nhất
của một mạng lưới thần kinh.

00:00:44.650 --> 00:00:47.040
Tôi sẽ định nghĩa
hàm dày đặc,

00:00:47.040 --> 00:00:48.755
lấy làm đầu vào

00:00:48.755 --> 00:00:51.545
kích hoạt từ
lớp trước,

00:00:51.545 --> 00:00:54.440
cũng như các tham số w và

00:00:54.440 --> 00:00:58.325
b cho các nơ-ron
trong một lớp nhất định.

00:00:58.325 --> 00:01:01.805
Sử dụng ví dụ từ
video trước,

00:01:01.805 --> 00:01:06.860
nếu lớp 1 có ba nơ-ron,

00:01:06.860 --> 00:01:13.610
và nếu w_1 và w_2
và w_3 là những thứ này,

00:01:13.610 --> 00:01:15.830
sau đó những gì chúng ta sẽ làm là

00:01:15.830 --> 00:01:19.699
xếp tất cả các
vectơ sóng này thành một ma trận.

00:01:19.699 --> 00:01:25.130
Đây sẽ là một
ma trận hai nhân ba,

00:01:25.130 --> 00:01:27.350
cột đầu tiên ở đâu

00:01:27.350 --> 00:01:30.725
tham số w_1,1
cột thứ hai

00:01:30.725 --> 00:01:31.910
là tham số w_1,

00:01:31.910 --> 00:01:36.440
2 và cột thứ ba
là tham số w_1,3.

00:01:36.440 --> 00:01:38.495
Sau đó, theo một cách tương tự,

00:01:38.495 --> 00:01:42.255
nếu bạn có tham số là,

00:01:42.255 --> 00:01:44.250
b_1,1 bằng âm một,

00:01:44.250 --> 00:01:46.500
b_1,2 bằng một, v.v.

00:01:46.500 --> 00:01:49.370
sau đó chúng ta sẽ cộng
ba số này thành

00:01:49.370 --> 00:01:52.535
mảng 1D b như sau,

00:01:52.535 --> 00:01:54.685
âm một, một, hai.

00:01:54.685 --> 00:01:57.470
Những gì chức năng dày đặc
sẽ làm là lấy như

00:01:57.470 --> 00:02:00.230
nhập kích hoạt
từ lớp trước,

00:02:00.230 --> 00:02:02.555
và a ở đây có thể là a_0,

00:02:02.555 --> 00:02:04.715
bằng với x,

00:02:04.715 --> 00:02:07.250
hoặc kích hoạt
từ lớp sau,

00:02:07.250 --> 00:02:12.890
cũng như các tham số w được
xếp chồng lên nhau trong các cột,

00:02:12.890 --> 00:02:14.120
như hình bên phải,

00:02:14.120 --> 00:02:16.325
cũng như các tham số b

00:02:16.325 --> 00:02:19.645
cũng được xếp chồng lên nhau thành một mảng 1D,

00:02:19.645 --> 00:02:22.865
như thể hiện ở
bên trái đằng kia.

00:02:22.865 --> 00:02:28.150
Những gì chức năng này
sẽ làm là nhập một vào

00:02:28.150 --> 00:02:30.260
kích hoạt từ
lớp trước và sẽ

00:02:30.260 --> 00:02:34.015
xuất các kích hoạt
từ lớp hiện tại.

00:02:34.015 --> 00:02:37.440
Hãy bước qua
mã để làm điều này.

00:02:37.440 --> 00:02:41.310
Đây là mã. Đầu tiên,
các đơn vị bằng W.shape,1.

00:02:41.310 --> 00:02:47.660
W ở đây là ma trận hai nhân ba,

00:02:47.660 --> 00:02:50.375
và do đó số
cột là ba.

00:02:50.375 --> 00:02:53.600
Điều đó bằng với số lượng
đơn vị trong lớp này.

00:02:53.600 --> 00:02:56.440
Ở đây, các đơn vị sẽ
bằng ba.

00:02:56.440 --> 00:02:58.585
Nhìn vào hình dạng của w,

00:02:58.585 --> 00:03:01.310
chỉ là một cách để
lấy ra số lượng

00:03:01.310 --> 00:03:05.335
đơn vị ẩn hoặc số lượng
đơn vị trong lớp này.

00:03:05.335 --> 00:03:08.570
Tiếp theo, chúng tôi đặt a thành một mảng

00:03:08.570 --> 00:03:12.155
số 0 có nhiều phần tử
bằng số đơn vị.

00:03:12.155 --> 00:03:14.180
Trong ví dụ này, chúng ta cần

00:03:14.180 --> 00:03:16.205
để xuất ba
giá trị kích hoạt,

00:03:16.205 --> 00:03:19.620
vì vậy điều này chỉ khởi tạo
a bằng 0,

00:03:19.620 --> 00:03:22.800
số không, số không, một dãy
ba số không.

00:03:22.800 --> 00:03:26.430
Tiếp theo, chúng ta thực hiện một
vòng lặp for để tính toán giá trị đầu tiên,

00:03:26.430 --> 00:03:28.600
phần tử thứ hai, thứ ba của a.

00:03:28.600 --> 00:03:30.350
Đối với j trong đơn vị phạm vi,

00:03:30.350 --> 00:03:33.665
vì vậy j đi từ không
đến đơn vị trừ đi một.

00:03:33.665 --> 00:03:34.850
Nó đi từ 0, 1,

00:03:34.850 --> 00:03:37.805
2 lập chỉ mục từ 0
và Python như bình thường.

00:03:37.805 --> 00:03:42.915
Lệnh này w bằng
W dấu hai chấm j,

00:03:42.915 --> 00:03:45.075
đây là cách bạn rút ra

00:03:45.075 --> 00:03:49.790
cột thứ j của
ma trận trong Python.

00:03:49.790 --> 00:03:52.115
Lần đầu tiên
thông qua vòng lặp này,

00:03:52.115 --> 00:03:55.325
điều này sẽ kéo
cột đầu tiên của w,

00:03:55.325 --> 00:03:58.550
và như vậy sẽ kéo ra w_1,1.

00:03:58.550 --> 00:04:00.430
Lần thứ hai
thông qua vòng lặp này,

00:04:00.430 --> 00:04:03.410
khi bạn đang tính toán
kích hoạt thiết bị thứ hai,

00:04:03.410 --> 00:04:06.920
sẽ kéo ra
cột thứ hai tương ứng với w_1,

00:04:06.920 --> 00:04:10.780
2, v.v. lần thứ ba
thông qua vòng lặp này.

00:04:10.780 --> 00:04:14.335
Sau đó, bạn tính z bằng
công thức thông thường,

00:04:14.335 --> 00:04:16.040
là một tích vô hướng giữa

00:04:16.040 --> 00:04:18.095
tham số đó w và

00:04:18.095 --> 00:04:20.480
kích hoạt mà
bạn đã nhận được,

00:04:20.480 --> 00:04:22.610
cộng b,j.

00:04:22.610 --> 00:04:25.430
Và sau đó bạn tính
kích hoạt a, j,

00:04:25.430 --> 00:04:29.035
bằng g
hàm sigmoid áp dụng cho z.

00:04:29.035 --> 00:04:31.220
Ba lần thông qua
vòng lặp này và bạn tính toán nó,

00:04:31.220 --> 00:04:33.455
các giá trị cho cả ba giá trị

00:04:33.455 --> 00:04:35.705
của vectơ
kích hoạt này là a.

00:04:35.705 --> 00:04:38.860
Sau đó, cuối cùng bạn trở lại a.

00:04:38.860 --> 00:04:41.375
Chức năng dày đặc
làm gì là nó

00:04:41.375 --> 00:04:43.880
nhập các kích hoạt
từ lớp trước,

00:04:43.880 --> 00:04:46.430
và đưa ra các tham số
cho lớp hiện tại,

00:04:46.430 --> 00:04:50.360
nó trả về các kích hoạt
cho lớp tiếp theo.

00:04:50.360 --> 00:04:52.055
Với chức năng dày đặc,

00:04:52.055 --> 00:04:53.330
đây là cách bạn có thể chuỗi

00:04:53.330 --> 00:04:56.780
cùng nhau tuần tự một vài
lớp dày đặc,

00:04:56.780 --> 00:04:59.990
để thực hiện chuyển tiếp
prop trong mạng lưới thần kinh.

00:04:59.990 --> 00:05:03.545
Với các tính năng đầu vào x,

00:05:03.545 --> 00:05:05.460
sau đó bạn có thể tính toán

00:05:05.460 --> 00:05:11.900
kích hoạt a_1 thành
a_1 bằng mật độ của x,

00:05:11.900 --> 00:05:14.490
w_1, b_1, ở đây w_1,

00:05:14.490 --> 00:05:16.565
b_1 là các tham số,

00:05:16.565 --> 00:05:18.365
đôi khi còn
được gọi là trọng lượng

00:05:18.365 --> 00:05:20.785
của lớp ẩn đầu tiên.

00:05:20.785 --> 00:05:26.385
Sau đó, bạn có thể tính toán a_2
dày đặc như bây giờ a_1,

00:05:26.385 --> 00:05:28.870
mà bạn vừa tính ở trên.

00:05:29.030 --> 00:05:32.330
W_2, b-2 là
các tham số hoặc

00:05:32.330 --> 00:05:35.020
trọng số của
lớp ẩn thứ hai này.

00:05:35.020 --> 00:05:38.490
Sau đó tính a_3 và a_4.

00:05:38.490 --> 00:05:42.080
Nếu đây là một
mạng lưới thần kinh có bốn lớp,

00:05:42.080 --> 00:05:46.610
sau đó xác định đầu ra f
của x chỉ bằng a_4,

00:05:46.610 --> 00:05:49.550
và vì vậy bạn trả về f của x.

00:05:49.550 --> 00:05:53.195
Lưu ý rằng ở đây tôi đang sử dụng W,

00:05:53.195 --> 00:05:55.759
bởi vì theo các
công ước công chứng

00:05:55.759 --> 00:05:58.490
từ đại số tuyến tính là
sử dụng chữ hoa hoặc

00:05:58.490 --> 00:06:01.070
một bảng chữ cái viết hoa là
khi nó đề cập đến

00:06:01.070 --> 00:06:05.630
một ma trận và chữ thường đề cập
đến vectơ và vô hướng.

00:06:05.630 --> 00:06:06.950
Bởi vì nó là một ma trận,

00:06:06.950 --> 00:06:09.990
đây là W. Thế thôi.

00:06:09.990 --> 00:06:11.540
Bây giờ bạn đã biết cách triển khai

00:06:11.540 --> 00:06:13.725
về phía trước chống đỡ cho mình
từ đầu.

00:06:13.725 --> 00:06:17.000
Bạn có thể xem tất cả mã này
và chạy nó và thực hành nó

00:06:17.000 --> 00:06:20.660
bản thân bạn trong phòng thí nghiệm thực hành
cũng sẽ làm được điều này.

00:06:20.660 --> 00:06:22.940
Tôi nghĩ rằng ngay cả
khi bạn đang sử dụng

00:06:22.940 --> 00:06:25.100
các thư viện mạnh mẽ
như TensorFlow,

00:06:25.100 --> 00:06:27.710
thật hữu ích khi biết
nó hoạt động như thế nào dưới mui xe.

00:06:27.710 --> 00:06:30.095
Bởi vì trong trường hợp
xảy ra sự cố,

00:06:30.095 --> 00:06:32.045
trong trường hợp thứ gì đó
chạy rất chậm,

00:06:32.045 --> 00:06:33.830
hoặc bạn có một kết quả kỳ lạ,

00:06:33.830 --> 00:06:35.585
hoặc có vẻ như có một lỗi,

00:06:35.585 --> 00:06:38.600
khả năng của bạn để hiểu
những gì đang thực sự xảy ra

00:06:38.600 --> 00:06:40.130
sẽ làm cho bạn
hiệu quả hơn nhiều

00:06:40.130 --> 00:06:41.690
khi gỡ lỗi mã của bạn.

00:06:41.690 --> 00:06:45.050
Khi tôi chạy các
thuật toán máy học rất nhiều lần,

00:06:45.050 --> 00:06:46.490
thành thật mà nói, nó không hoạt động.

00:06:46.490 --> 00:06:48.325
Sophie, không phải lần đầu.

00:06:48.325 --> 00:06:50.975
Tôi thấy rằng khả năng gỡ lỗi của tôi

00:06:50.975 --> 00:06:53.840
mã của tôi là mã TensorFlow
hoặc mã nào khác,

00:06:53.840 --> 00:06:55.580
là thực sự quan trọng để được

00:06:55.580 --> 00:06:58.195
một kỹ sư học máy hiệu quả
.

00:06:58.195 --> 00:06:59.945
Ngay cả khi bạn đang sử dụng

00:06:59.945 --> 00:07:02.015
TensorFlow hoặc một số
khuôn khổ khác,

00:07:02.015 --> 00:07:06.020
Tôi hy vọng rằng bạn thấy
sự hiểu biết sâu sắc hơn này hữu ích cho

00:07:06.020 --> 00:07:08.570
các ứng dụng của riêng bạn
và để gỡ lỗi

00:07:08.570 --> 00:07:11.980
các thuật toán máy học của riêng bạn .

00:07:11.980 --> 00:07:15.545
Đó là nó. Đó là
video cần thiết cuối cùng

00:07:15.545 --> 00:07:17.615
của tuần này với mã trong đó.

00:07:17.615 --> 00:07:18.950
Trong video tiếp theo,

00:07:18.950 --> 00:07:20.720
Tôi muốn đi sâu
vào những gì tôi nghĩ là

00:07:20.720 --> 00:07:22.910
một chủ đề thú vị và hấp dẫn
, đó là,

00:07:22.910 --> 00:07:25.715
mối quan hệ
giữa mạng lưới thần kinh và

00:07:25.715 --> 00:07:30.145
AI hay AGI, trí tuệ nhân tạo
nói chung?

00:07:30.145 --> 00:07:32.045
Đây là một chủ đề gây tranh cãi,

00:07:32.045 --> 00:07:34.235
nhưng bởi vì nó đã được
thảo luận rộng rãi,

00:07:34.235 --> 00:07:37.270
Tôi muốn chia sẻ với bạn
một số suy nghĩ về điều này.

00:07:37.270 --> 00:07:39.855
Khi bạn được hỏi,

00:07:39.855 --> 00:07:41.915
đều là mạng lưới thần kinh

00:07:41.915 --> 00:07:44.180
trên con đường đạt đến
trí thông minh cấp độ con người?

00:07:44.180 --> 00:07:47.555
Bạn có một khuôn khổ để
suy nghĩ về câu hỏi đó.

00:07:47.555 --> 00:07:49.970
Chúng ta hãy xem
chủ đề thú vị đó,

