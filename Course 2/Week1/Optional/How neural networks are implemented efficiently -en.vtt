WEBVTT

1
00:00:00.680 --> 00:00:04.200
One of the reasons that
deep learning researchers

2
00:00:04.200 --> 00:00:07.080
have been able to scale
up neural networks,

3
00:00:07.080 --> 00:00:09.195
and thought really
large neural networks

4
00:00:09.195 --> 00:00:10.545
over the last decade,

5
00:00:10.545 --> 00:00:13.680
is because neural networks
can be vectorized.

6
00:00:13.680 --> 00:00:15.690
They can be implemented very

7
00:00:15.690 --> 00:00:18.810
efficiently using
matrix multiplications.

8
00:00:18.810 --> 00:00:20.415
It turns out that

9
00:00:20.415 --> 00:00:23.730
parallel computing
hardware, including GPUs,

10
00:00:23.730 --> 00:00:26.400
but also some CPU
functions are very

11
00:00:26.400 --> 00:00:29.910
good at doing very large
matrix multiplications.

12
00:00:29.910 --> 00:00:32.340
In this video, we'll
take a look at how

13
00:00:32.340 --> 00:00:36.300
these vectorized implementations
of neural networks work.

14
00:00:36.300 --> 00:00:38.990
Without these ideas, I
don't think deep learning

15
00:00:38.990 --> 00:00:42.040
would be anywhere near a
success and scale today.

16
00:00:42.040 --> 00:00:45.500
Here on the left is the
code that you had seen

17
00:00:45.500 --> 00:00:50.300
previously of how you would
implement forward prop,

18
00:00:50.300 --> 00:00:51.830
or forward propagation,

19
00:00:51.830 --> 00:00:54.030
in a single layer.

20
00:00:54.110 --> 00:00:57.585
X here is the input,

21
00:00:57.585 --> 00:01:01.140
W, the weights of
the first, second,

22
00:01:01.140 --> 00:01:03.630
and third neurons, say,

23
00:01:03.630 --> 00:01:05.570
parameters B, and then this

24
00:01:05.570 --> 00:01:08.150
is the same code as
which we saw before.

25
00:01:08.150 --> 00:01:10.790
This will output three numbers,

26
00:01:10.790 --> 00:01:11.930
say, like that.

27
00:01:11.930 --> 00:01:14.690
If you actually implement
this computation,

28
00:01:14.690 --> 00:01:17.045
you get 1, 0, 1.

29
00:01:17.045 --> 00:01:19.805
It turns out you can develop

30
00:01:19.805 --> 00:01:25.540
a vectorized implementation
of this function as follows.

31
00:01:25.540 --> 00:01:29.310
Set X to be equal to this.

32
00:01:29.310 --> 00:01:31.560
Notice the double
square brackets.

33
00:01:31.560 --> 00:01:35.764
This is now a 2D array,
like in TensorFlow.

34
00:01:35.764 --> 00:01:38.250
W is the same as before,

35
00:01:38.250 --> 00:01:41.370
and B, I'm now using B,

36
00:01:41.370 --> 00:01:47.970
is also a one by three 2D array.

37
00:01:47.970 --> 00:01:51.405
Then it turns out that
all of these steps,

38
00:01:51.405 --> 00:01:53.190
this for loop inside,

39
00:01:53.190 --> 00:01:56.280
can be replaced with just
a couple of lines of

40
00:01:56.280 --> 00:02:00.355
code, Z equals np.matmul.

41
00:02:00.355 --> 00:02:04.710
Matmul is how NumPy carries
out matrix multiplication.

42
00:02:04.710 --> 00:02:08.565
Where now X and W
are both matrices,

43
00:02:08.565 --> 00:02:11.560
and so you just
multiply them together.

44
00:02:11.560 --> 00:02:14.805
It turns out that this for loop,

45
00:02:14.805 --> 00:02:16.230
all of these lines
of code can be

46
00:02:16.230 --> 00:02:18.435
replaced with just a
couple of lines of code,

47
00:02:18.435 --> 00:02:23.440
which gives a vectorized
implementation of this function.

48
00:02:23.560 --> 00:02:25.710
You compute Z,

49
00:02:25.710 --> 00:02:27.185
which is now a matrix again,

50
00:02:27.185 --> 00:02:32.700
as numpy.matmul
between A in and W,

51
00:02:32.700 --> 00:02:36.485
where here A in and
W are both matrices,

52
00:02:36.485 --> 00:02:38.719
and matmul is how NumPy

53
00:02:38.719 --> 00:02:41.660
carries out a matrix
multiplication.

54
00:02:41.660 --> 00:02:43.600
It multiplies two
matrices together,

55
00:02:43.600 --> 00:02:46.120
and then adds the
matrix B to it.

56
00:02:46.120 --> 00:02:52.030
Then A out is equal to the
activation function g,

57
00:02:52.030 --> 00:02:53.530
that is the sigmoid function,

58
00:02:53.530 --> 00:02:57.415
applied element-wise
to this matrix Z,

59
00:02:57.415 --> 00:03:00.600
and then you finally
return A out.

60
00:03:00.600 --> 00:03:03.995
This is what the
code looks like.

61
00:03:03.995 --> 00:03:07.225
Notice that in the
vectorized implementation,

62
00:03:07.225 --> 00:03:09.340
all of these quantities, x,

63
00:03:09.340 --> 00:03:12.895
which is fed into the value
of A in as well as W, B,

64
00:03:12.895 --> 00:03:15.145
as well as Z and A out,

65
00:03:15.145 --> 00:03:17.620
all of these are now 2D arrays.

66
00:03:17.620 --> 00:03:19.900
All of these are matrices.

67
00:03:19.900 --> 00:03:22.870
This turns out to be

68
00:03:22.870 --> 00:03:25.420
a very efficient
implementation of

69
00:03:25.420 --> 00:03:27.265
one step of forward propagation

70
00:03:27.265 --> 00:03:29.445
through a dense layer
in the neural network.

71
00:03:29.445 --> 00:03:31.300
This is code for

72
00:03:31.300 --> 00:03:33.385
a vectorized implementation of

73
00:03:33.385 --> 00:03:35.725
forward prop in a
neural network.

74
00:03:35.725 --> 00:03:37.420
But what is this code

75
00:03:37.420 --> 00:03:39.625
doing and how does
it actually work?

76
00:03:39.625 --> 00:03:42.960
What is this matmul
actually doing?

77
00:03:42.960 --> 00:03:45.140
In the next two videos,

78
00:03:45.140 --> 00:03:46.685
both also optional,

79
00:03:46.685 --> 00:03:50.800
we'll go over matrix
multiplication and how that works.

80
00:03:50.800 --> 00:03:53.440
If you're familiar
with linear algebra,

81
00:03:53.440 --> 00:03:57.325
if you're familiar with
vectors, matrices, transposes,

82
00:03:57.325 --> 00:03:59.500
and matrix multiplications,

83
00:03:59.500 --> 00:04:02.440
you can safely just
quickly skim over

84
00:04:02.440 --> 00:04:06.210
these two videos and jump to
the last video of this week.

85
00:04:06.210 --> 00:04:09.550
Then in the last video of
this week, also optional,

86
00:04:09.550 --> 00:04:12.040
we'll dive into more
detail to explain how

87
00:04:12.040 --> 00:04:15.955
matmul gives you this
vectorized implementation.

88
00:04:15.955 --> 00:04:18.205
Let's go onto the next video,

89
00:04:18.205 --> 00:04:22.340
where we'll take a look at
what matrix multiplication is.